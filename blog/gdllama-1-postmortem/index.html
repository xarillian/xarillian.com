<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>xarillian - GDLlama 1.0 Postmortem</title>

  <link rel="stylesheet" href="/styles/bundle.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css">
  <link rel="icon" type="image/png" href="favicon.png"/>

  <meta property="og:url" content="https://xarillian.com">
  <meta property="og:site_name" content="xarillian">

  
  <meta name="description" content="Reflections on the development of GDLlama 1.0.0">
  <meta property="og:type" content="article">
  <meta property="og:title" content="xarillian - GDLlama 1.0 Postmortem">
  <meta property="og:description" content="Reflections on the development of GDLlama 1.0.0">

</head>
<body>
  <header>
  <div class="header-content">
    <div class="header-title-container no-link-formatting">
      <h1 data-text="xarillian">
        <a href="/" aria-label="Xarillian - Home Page" title="/zæˈrɪliən/">
          xarillian
        </a>
      </h1>
      <span class="tagline skip-mobile" id="quote">it's not the user's fault</span>
    <script>
      const quotes = ["it\u0027s not the user\u0027s fault", "it\u0027s not the mountains we conquer, but ourselves", "a ship in a harbor is safe, but that is not what ships are built for", "everything excellent is as difficult as it is rare", "you can just do things", "reach heaven by violence!", "I like this player. It played well. It did not give up.", "...the player believed the universe had spoken to it through the zeroes and ones"];
      document.addEventListener('DOMContentLoaded', () => {
        const el = document.getElementById('quote');
        el.textContent = quotes[Math.floor(Math.random() * quotes.length)];
      });
    </script>
    </div>
    <nav class="main-nav">
      <a href="/blog" class="styled-link">Blog</a>
      <a href="/tags" class="styled-link">Tags</a>
      <a href="/about" class="styled-link">About</a>
    </nav>
  </div>
</header>
  <main>
    

  <div>
    
  </div>

  <span class="blog-post">
    <h1 class="blog-post">GDLlama 1.0 Postmortem</h1>
    
      
        <p class="blog-post-date">October 19, 2025</p>
      
    
    

    <div class="blog-post">
      <p><a href="https://github.com/xarillian/GDLlama">GDLlama</a> is a GDExtension that allows a Godot developer to use the powerful <code>llama.cpp</code> library to run GGUF models in Godot games. When I started this project, I wanted to use Gemma 3 270m in my own Godot projects; that's a tiny model and it runs really well and I was excited to see how it performed. However, I quickly found that existing solutions for Godot were out of date, difficult to use, or both. So, I wanted to take a crack at it. GDLlama lets developers add AI-powered features to their games. You could do something with dialogue generation, or radiant quest creation. Have your NPCs respond dynamically to player actions, or create a DM for a virtual gaming experience that would have been impossible just a few years ago. The nascent space is just so vast and exciting, and I am so happy to contribute to it.</p>

<p>This was originally a fork of <a href="https://github.com/Adriankhl/godot-llm">godot-llm</a> by Adriankhl; I wanted to update his work to the latest version of <code>llama.cpp</code> so that I could use Google's new <a href="https://huggingface.co/unsloth/gemma-3-270m-it-GGUF/tree/main">Gemma 3 270m model</a> in my own projects, and I figured others might find it useful too. When I started working on the update, I found that there were quite a few issues with the original codebase that made it difficult to work with, so I ended up rewriting large portions of it. After a few months of development (I think I started in the end of July 2025), I tagged a stable 1.0.0 release.</p>

<p>The process of developing GDLlama went, admittedly, smoother than I expected despite my lack of initial planning. I had no experience with GDExtensions, only university experience with C++, and I was nervous to look at one codebase someone else had written. Let alone three of them! After a bit of fumbling around, resolving linting errors one at a time, then build errors, then Godot integration issues, until I was able to get a sort-of working version of the original project running with the latest <code>llama.cpp</code>. At that point, I realized a few things: one, not all of the access methods were working and I had no idea why; two, some of the features godot-llm implemented were no longer supported by <code>llama.cpp</code> or were implemented in a different way; and three, the code was a bit of a mess. Not knowing much about <code>llama.cpp</code> or Godot, I decided to start with the tertiary goal first: clean up the code. That was something I knew I could do.</p>

<h2>Refactoring</h2>

<p>To start, I needed to figure out what GDLlama wanted to be. At that point, there were four key pieces of access:</p>

<ul>
<li><code>gdllama</code>: for LLM inference</li>
<li><code>gdllava</code>: for vision models</li>
<li><code>gdembeddings</code>: for embedding models</li>
<li><code>llm_db</code>: an SQLite database for managing model metadata and state</li>
</ul>

<p>and three corresponding runners:</p>

<ul>
<li><code>llama_runner</code></li>
<li><code>llava_runner</code></li>
<li><code>embedding_runner</code></li>
</ul>

<p>On top of this, I found that godot-llm was loading and unloading a model with every inference call, which was not ideal. So, I decided to focus on the <code>gdllama</code> portion and create a single, stable base which would function as the core access point for the extension: the <code>GDLlama</code> node. This single node would replace the previous <code>gdllama</code>, <code>gdllava</code>, and <code>gdembeddings</code> nodes, as well as the runners. This would simplify the codebase significantly and make it easier to maintain, as well as simplify the user experience. There was simply one node to learn, one node to use. Need inference? Load a GDLlama node, load a model, and call an inference method. Need embeddings? Same process. Vision models? Ehh, not implemented, but eventually a GDLlama node!</p>

<p>I knew the <code>llama_runner</code> was a well-separated piece of code that handled the core inference logic. I just needed it to, y'know, <em>work</em>. And embeddings, too! Embeddings had to be added in before I could have a solid 1.0 release, since they enabled tool calling, token manipulation, semantic search, and other really powerful features. I also realized that both <code>llama_runner</code> and <code>gdllama</code> were doing too much between the two of them. <code>gdllama</code> contained access methods, model management, llama/godot communication, and <code>llama_runner</code> had its fingers in generation, state management, and even parts of the model lifecycle. It was a tangled mess that made debugging a nightmare.</p>

<p>So, I figured I wanted four main pieces, each with a  single responsibility, which conveniently fit into a nice architecture:</p>

<ul>
<li><code>gdllama.cpp</code>: Lives in Godot-land. Responsible for exposing access methods to Godot and handling Godot-specific logic and types.</li>
<li><code>llama_controller.cpp</code>: The orchestrator. Manages conversation history, coordinates between the runner and state, and handles the distinction between single-shot and continuous chat generation.</li>
<li><code>llama_state.cpp</code>: The lifecycle manager. Responsible for loading and unloading models, maintaining the llama context, and managing backend initialization.</li>
<li><code>llama_runner.cpp</code>: The execution engine. Handles the low-level prediction and embedding generation loops, including tokenization, sampling, and decoding.</li>
</ul>

<p>Once I knew this structure, I set about creating <em>a plan</em>. That was pretty crazy, but it really helped me stay on track. In GitHub, I created a milestone for the 1.0 release and stocked it full of issues that I wanted to tackle. There was no real timeline, no real distinction between issue sizes, just stuff I felt would be fine to tackle all at once and get me to something I could call done. I started with logging, since that was a pain point for me and I didn't know how to debug in C++ at that time. Everything got logged. It was crazy back then. So much logging... but it worked and it helped me find issues very quickly. Some amount of that logging still lives as debug logs, which can be enabled by compiling a debug build!</p>

<p>From there, I was systematic. I tackled model loading and unloading, and built the structure I wanted for the project in a nebulously named and large change, "Simplify Generation". I hit documentation, build steps, embeddings, removing SQL, and so on. Each issue felt like a small victory. Some issues felt huge! When I finally got internal documentation working, I was ecstatic. That alone was days spent Googling, trial and error, reading CMake docs, and finally figuring out how to get internal docs working (I had to increment the Godot version, something I really wanted to avoid... but c'est la vie). When I finished, I ran around the house and told my partner all about it. That was a good day.</p>

<h3>The Runner</h3>

<p>The heart of the refactor had to revolve around the <code>llama_runner</code>. It did everything and it needed to be re-written completely from scratch. The original runner was a single 1000+ line function that handled prompt evaluation, context shifting, different custom modes, and session management all in one loop. Debugging meant scrolling through endless conditionals trying to figure out which state the generator was in. It felt easy to break and hard to verify. It was also using deprecated or remove APIs from <code>llama.cpp</code>, which meant the initial pass had to be an update.</p>

<p>The new runner separates the prompt evaluation phase from the generation phase cleanly. During evaluation, we batch process tokens from the prompt using llama<em>batch</em>get<em>one, respecting the n</em>batch parameter for optimal throughput. Once we're past the prompt, the generation loop becomes straightforward: sample, accept, decode, repeat. The key point is that the new structure is way easier to read, maintain, and debug. Each phase is clear, and the state transitions are explicit.</p>

<p>Embedding support was important for me to implement in the initial 1.0 version. It was implemented as a separate function in the <code>llama_runner</code>. While developing it, I stumbled upon pooling strategies and realized the project needed to support multiple. The runner detects whether a model uses last-token pooling or built-in pooling (mean, CLS, etc.) and handles each appropriately. This was critical for enabling semantic search and tool-calling features for various models; for testing, I used the Gemma series both for embedding and inference. </p>

<p>It's not perfect, but it's solid, and goddamn is it maintainable.</p>

<p>...And, actually, while I write this I think I found a bug in the current implementation. Oops. Well, a 1.0.1 will fix that!</p>

<h2>Testing, Docs, Distribution</h2>

<p>I also set up a proper test suite that runs via <code>gtest</code> in debug builds. Nothing fancy, but it gives me confidence when making changes. Future me I'm sure will appreciate it! On top of this, I set up a continuous integration via GitHub Actions to pull down and compile against the latest version of <code>llama.cpp</code> weekly, ensuring that I at least know when breaking changes happen upstream. This suite ended up giving me some grief after incrementing the Godot version. It turned out, Godot's cmake setup vehemently dislikes exception handling, which is something I was doing quite a bit of in the codebase; nothing that touched Godot, but still. I couldn't figure out how to disable build errors for exception handling in a way that wouldn't override the Godot build settings, so I ended up removing exceptions entirely from the project. Errors would instead bubble up from wherever they occurred, and the project would either log or handle them based on the existence of a log message. That was a pain, and done perhaps in a rush, but it is what it is.</p>

<p>Perhaps controversially, I decided to remove the SQLite database component. While it was a neat thing to have, it introduced a <em>lot</em> of complexity to what was otherwise a fairly straightforward extension. It felt like an extension on top of the extension, and I wanted development and maintenance time focused on the cool and fun functionality. While working on the project, <a href="https://github.com/xarillian/GDLlama/blob/master/docs/DESIGN.md">I was thinking of what principles I wanted to follow</a>. One of these was User Agency, styled as "the user should be able to f*ck up." By removing the in-built database, users are free to implement state management solutions in whatever way they see fit without dragging a GDLlama db along; those could be in-memory arrays, JSON, external databases — whatever. The point was, GDLlama didn't need to care. It should work with the user rather than prescribe data management solutions.</p>

<p>Am I entirely happy about that state? Sure, why not. Logging is still tightly coupled to Godot, so <code>gdllama</code> isn't entirely quarantining Godot code. The current structure makes future integrations a touch difficult, though I think I can thread them from <code>gdllama.cpp</code> to an appropriate controller. Much of the code could be way more modular, but I'll get to that as it becomes more important with future updates. Ultimately, it gets the job done and it's a solid foundation to build on. Works for me!</p>

<p>I spent, what some might call, an inordinate amount of time on documentation. I really hate projects that don't help the user get started, and I really love technical writing — what a delicious combination of traits! So, I wrote a lot of documentation. There are build instructions (important, must have), an API reference (important, must have), a legal guide (sure?), a document on different architecture (okay), a design principles document, and ... I think that's it? I also wrote a few examples to help users get started quickly. I hope it helps! <a href="https://github.com/xarillian/GDLlama/tree/master/docs">Docs, find 'em here.</a></p>

<p>Out of everything I did, I can't help but laugh about how long I've thought about distribution. I know I wanted to give back to the community, and the project is MIT licensed (and needs to be, given I adopted it from godot-llm). Seriously, after all the hard, technical work in the <code>llama</code> mines, it ends up being distribution that feels the hardest. Talk about a lesson learned! I think what I want is a two-tier system: free access to the source code and build instructions via GitHub, and a paid prebuilt extension via the Godot Marketplace for convenience and a promise of future maintenance. That feels like a fair balance between open source and something I can sustain working on. We'll see how that goes!</p>

<p>The strangest thing about this whole process was the end. I found that when I went to hit that final merge that I didn't want to do it. My mouse hovered over that button and retreated so that I could put one more bit of polish on the code, or so that I could go for a run, or anything but hit that button. It was a strange feeling. I didn't want to be done. I didn't want to put it out there and say, "Here it is, it's ready. Come use it!" I think part of it was that I was nervous about putting my work out there for others to use. What if it had bugs? What if it wasn't as good as I thought it was? Ultimately, I don't think it matters. I did my best. It's out there. Anything that's wrong can be fixed. A good start!</p>

<h2>Slop?</h2>

<p>I mean, yeah probably. I worry a bit about slop and sloppy content. Well, really, I think that's a taste issue. I built this because I wanted to make games that weren't possible before. My hope is that developers use it to enhance their games in novel and exciting ways, rather than as a crutch to avoid writing engaging content. That's the use case I'm optimizing for: creative people who care about their craft. If you're using GDLlama, make something you'd genuinely want to play. Think about the content you already love and iterate. Be virtuous and make something you love.</p>

<h2>Future Plans</h2>

<p>Ah, but work is never done. There are a few features I want to add in the future. A request has already come in for adding support for batch processing that I'd like to get to before anything else. LoRA support is another feature that I'd love to add sooner rather than later, since it would open development possibilities that I'd really like to explore myself. I need to hit other integrations (not just Llama ... and that likely means a rename!), allow better context manipulation, and so on, and so on. my continually growing list of features to add is available on the <a href="https://github.com/xarillian/GDLlama/issues">GitHub issues page</a>.</p>

<p>And then, when all is said and done, I want to make something with it. My own game ideas have been on hold while I worked on GDLlama, and I'm eager to get back to them; I'm proud that my work on GDLlama can help me make those ideas a reality. I'm happy to get those ideas out of my notes app! And of course, I hope that others will make great games with GDLlama too.</p>

    </div>
  </span>

  
    <p>This entry was tagged
      
        
        <a href="/tags/gamedev" class="post-tag">gamedev</a>
        
      
    </p>
  



  </main>
  
</body>
</html>